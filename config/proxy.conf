[global]
listen_port = 8080
threads = 4
strategy = roundrobin
log_level = INFO
io_model = epoll
reuse_port = 0

[tls]
# TLS termination (opt-in).
# When enabled, the listener accepts both HTTPS and plain HTTP by sniffing the first byte.
# Provide a certificate (full chain) and private key in PEM format.
enable = 0
cert_path =
key_path =
# ACME HTTP-01 challenge directory (optional):
# If set, proxy serves: /.well-known/acme-challenge/<token> from this directory.
acme_challenge_dir =

[memory]
# Optional hugepage/THP hint for memory pool slabs (best-effort).
# hugepage=1 will round slab chunks to 2MB and call MADV_HUGEPAGE on them.
hugepage = 0
# Slab chunk size in KB (default 64; recommended 2048 when hugepage=1).
slab_chunk_kb = 64
#
# Buddy allocator for large blocks (best-effort, with LRU reclaim of idle arenas).
buddy_enable = 1
buddy_min_kb = 128
buddy_arena_kb = 8192
buddy_keep_arenas = 1
buddy_max_arenas = 8

[connection_limit]
max_total = 0
max_per_ip = 0
max_per_user = 0
user_header = X-Api-Token
user_max_entries = 10000
max_per_service = 0
service_max_entries = 10000
idle_timeout_sec = 0
cleanup_interval_sec = 1.0

[health_check]
mode = tcp
http_host = 127.0.0.1
http_path = /health
# Optional: user-defined script command (mode=script).
# Placeholders: {ip} {port}. Exit code 0 => healthy.
# Example: script_cmd = /bin/sh -c 'nc -z {ip} {port}'
script_cmd =
interval = 5.0
timeout = 2.0

[ai_check]
enable = 0
http_host = 127.0.0.1
http_path = /ai/status
interval = 5.0
timeout = 2.0

[batch]
# Batch processing optimization (opt-in).
# When enabled, eligible JSON POST requests to configured paths may be merged into one batched request:
# - Request body: [req1, req2, ...]
# - Response body: [resp1, resp2, ...] (same order/size), then split back to clients.
enable = 0
window_ms = 2
max_batch_size = 8
max_batch_bytes = 262144
max_response_bytes = 1048576
# Comma-separated exact paths eligible for batching (empty => all paths eligible).
paths =
# If require_header=1, only requests with header_name truthy (1/true/yes) are batched.
require_header = 0
header_name = X-Batch

[warmup]
# Warmup management: when a new backend becomes online, trigger a model preload request
# and keep it out of rotation until warmup succeeds.
enable = 0
model =
http_host = 127.0.0.1
http_path = /ai/warmup
timeout = 2.0

[service_discovery]
auto_weight = 0
provider = off
interval = 5.0
timeout = 2.0
default_weight = 1
consul_url = http://127.0.0.1:8500
consul_service =
consul_passing_only = 1
etcd_url = http://127.0.0.1:2379
etcd_key =
k8s_url =
k8s_token =
k8s_namespace = default
k8s_endpoints =

[udp]
listen_port = 0
idle_timeout_sec = 10.0
cleanup_interval_sec = 1.0

[l4]
# Raw TCP tunnel (opt-in). Useful for throughput/iperf3 testing.
# When enabled, proxy listens on listen_port and forwards bytes without HTTP parsing.
enable = 0
listen_port = 0

[session_affinity]
mode = none
header_name =
cookie_name =

[access_control]
ip_mode = off
cidrs =
require_token = 0
token_header = X-Api-Token
valid_tokens =
require_api_key = 0
api_key_header = X-Api-Key
valid_api_keys =

[audit_log]
path =

[rate_limit]
qps = 0
burst = 0
per_ip_qps = 0
per_ip_burst = 0
per_ip_idle_sec = 60.0
per_ip_max_entries = 10000
per_path_qps = 0
per_path_burst = 0
per_path_idle_sec = 60.0
per_path_max_entries = 10000

[congestion]
# Congestion control (sliding window + AIMD) for backend forwarding concurrency.
# enable=1 will limit the number of in-flight backend requests.
enable = 0
initial_window = 64
min_window = 1
max_window = 1024
additive_increase = 1
multiplicative_decrease = 0.7

[alerts]
enable = 0
interval_sec = 1.0
cooldown_sec = 30.0
merge_window_sec = 0.2
anomaly_enable = 0
anomaly_z = 3.0
anomaly_alpha = 0.2
anomaly_min_samples = 10
webhook_url =
sms_webhook_url =
email_smtp_host =
email_smtp_port = 25
email_from =
email_to =
email_subject_prefix = Proxy Alert
max_active_connections = -1
max_cpu_pct = -1
max_rss_mb = -1
max_fd_count = -1
max_backend_error_rate = -1

[history]
# Historical metrics sampling (opt-in).
# Exposes:
#   GET /history?seconds=60
#   GET /history/summary?seconds=300
enable = 0
sample_ms = 1000
max_points = 3600
persist_path =

[plugins]
# Plugin manager (dlopen shared libraries). Intended for pluggable extensions.
enable = 0
# Comma-separated plugin shared library paths (.so). Example:
# paths = ./build/plugins/libproxy_example_plugin.so
paths =
# Only dispatch requests whose path starts with these prefixes (comma-separated).
http_prefixes = /plugin

[ddos]
accept_qps = 0
accept_burst = 0
per_ip_accept_qps = 0
per_ip_accept_burst = 0
per_ip_idle_sec = 60.0
per_ip_max_entries = 10000

[mirror]
# Traffic mirroring (best-effort, non-blocking): send JSON events via UDP.
enable = 0
udp_host = 127.0.0.1
udp_port = 0
sample_rate = 1.0
max_bytes = 4096
max_body_bytes = 1024
include_req_body = 1
include_resp_body = 0

[cache]
# Cache integration (best-effort): Redis or Memcached backend.
# Only caches simple GET responses when Accept-Encoding=identity.
enable = 0
backend = off
host = 127.0.0.1
port = 0
ttl_sec = 60
timeout_ms = 5
max_value_bytes = 262144

[priority]
# Connection scheduling (opt-in): choose one of:
#   mode=priority : strict priority (default)
#   mode=fair     : fair queuing across flows
#   mode=edf      : earliest deadline first
#
# Scheduling is used only when max_inflight > 0.
enable = 0
mode = priority
max_inflight = 0
high_threshold = 8
low_delay_ms = 0
header_name = X-Priority
query_name = priority
flow_header_name = X-Flow
flow_query_name = flow
deadline_header_name = X-Deadline-Ms
deadline_query_name = deadline_ms
default_deadline_ms = 60000

# [rewrite:1]
# path_prefix = /api
# method = POST
# req_set_headers = X-Req-Added: 1
# req_del_headers = X-Req-Remove
# req_body_replace = PING=>PONG;foo=>bar
# resp_set_headers = X-From-Proxy: 1
# resp_del_headers = X-Backend
# resp_body_replace = HELLO=>WORLD

[backend:1]
ip = 127.0.0.1
port = 9001
weight = 1

[backend:2]
ip = 127.0.0.1
port = 9002
weight = 2
