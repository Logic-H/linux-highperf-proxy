{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RTX 远程压测（从本机打到 RTX）\n",
        "\n",
        "这个 Notebook 用于在“本机/压测机”远程压测 RTX 上运行的 Proxy，并生成 `project.txt` 里常见的指标：\n",
        "\n",
        "- **10K 并发连接**（`connect_hold`）\n",
        "- **HTTP /stats 延迟与 QPS**（`http_stats`）\n",
        "\n",
        "依赖：仓库自带脚本 `scripts/benchmark.py` / `scripts/load_test.py`。\n",
        "\n",
        "提示：10K 连接需要较高 fd 上限；请在启动 Jupyter 前执行：\n",
        "\n",
        "```bash\n",
        "ulimit -n 200000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, json, time, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "import resource\n",
        "\n",
        "# 目标 RTX（按需修改）\n",
        "RTX_HOST = \"10.200.98.98\"\n",
        "\n",
        "# RTX 上 proxy 的端口（常规部署）\n",
        "PROXY_PORT = 8080\n",
        "\n",
        "# 如果你在 RTX 上跑 ai_demo（端口默认 18080），可用于“巨大计算需求”演示\n",
        "AI_DEMO_PORT = 18080\n",
        "\n",
        "# 输出目录\n",
        "OUT_DIR = Path(\"bench_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"OUT_DIR=\", OUT_DIR.resolve())\n",
        "print(\"Python=\", sys.version.split()[0])\n",
        "print(\"Repo=\", Path('.').resolve())\n",
        "\n",
        "nofile_soft, nofile_hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
        "print(f\"RLIMIT_NOFILE soft={nofile_soft} hard={nofile_hard}\")\n",
        "if nofile_soft < 20000:\n",
        "    print(\"WARNING: 当前进程 fd 上限偏低；10K 并发连接可能失败。建议退出后用更高 ulimit 启动 Jupyter。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 快速探活：拉取 `/stats`\n",
        "\n",
        "确认本机到 RTX 网络可达、并且 proxy 正在运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "def get_json(url: str, timeout: float = 2.0):\n",
        "    with urllib.request.urlopen(url, timeout=timeout) as r:\n",
        "        data = r.read()\n",
        "    return json.loads(data.decode('utf-8'))\n",
        "\n",
        "stats_url = f\"http://{RTX_HOST}:{PROXY_PORT}/stats\"\n",
        "j = get_json(stats_url, timeout=3.0)\n",
        "print(\"OK /stats\")\n",
        "print(\"uptime_sec:\", j.get(\"uptime_sec\"))\n",
        "print(\"active_connections:\", j.get(\"active_connections\"))\n",
        "print(\"avg_qps:\", j.get(\"avg_qps\"))\n",
        "print(\"io:\", j.get(\"io\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 10K 并发连接压测（connect_hold）\n",
        "\n",
        "这个测试从本机建立大量 TCP 连接到 RTX 的 proxy，并保持一段时间。\n",
        "\n",
        "参数建议：\n",
        "- `--total 10000`：目标连接数\n",
        "- `--concurrency 2000`：建立连接的并发度（过大可能让本机端口/FD/网络瞬间吃满）\n",
        "- `--hold 20`：保持时间（秒）\n",
        "\n",
        "同时你可以打开 RTX 上的仪表盘看 `active_connections`：\n",
        "- `http://10.200.98.98:8080/dashboard`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_cmd(cmd: list[str], timeout_s: float | None = None) -> str:\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout_s)\n",
        "    print(p.stdout)\n",
        "    p.check_returncode()\n",
        "    return p.stdout\n",
        "\n",
        "out_json = OUT_DIR / \"remote_conn_hold_10k.json\"\n",
        "cmd = [\n",
        "    sys.executable, \"scripts/benchmark.py\",\n",
        "    \"--host\", RTX_HOST,\n",
        "    \"--port\", str(PROXY_PORT),\n",
        "    \"--bench\", \"connect_hold\",\n",
        "    \"--total\", \"10000\",\n",
        "    \"--concurrency\", \"2000\",\n",
        "    \"--hold\", \"20\",\n",
        "    \"--timeout\", \"3\",\n",
        "    \"--global-timeout\", \"90\",\n",
        "    \"--output\", str(out_json),\n",
        "]\n",
        "run_cmd(cmd, timeout_s=120)\n",
        "print(\"saved:\", out_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = json.loads(out_json.read_text(encoding='utf-8'))\n",
        "print(json.dumps(data, ensure_ascii=False, indent=2)[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) /stats 延迟与吞吐压测（http_stats）\n",
        "\n",
        "这个测试会高并发请求 RTX 的 `GET /stats`，输出 p50/p90/p99 延迟和 QPS。\n",
        "\n",
        "注意：`/stats` 不是数据面业务请求，但适合做“可观测接口在高频拉取下是否稳定”的基准。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_json = OUT_DIR / \"remote_http_stats.json\"\n",
        "cmd = [\n",
        "    sys.executable, \"scripts/benchmark.py\",\n",
        "    \"--host\", RTX_HOST,\n",
        "    \"--port\", str(PROXY_PORT),\n",
        "    \"--bench\", \"http_stats\",\n",
        "    \"--total\", \"20000\",\n",
        "    \"--concurrency\", \"400\",\n",
        "    \"--timeout\", \"2\",\n",
        "    \"--global-timeout\", \"120\",\n",
        "    \"--output\", str(out_json),\n",
        "]\n",
        "run_cmd(cmd, timeout_s=150)\n",
        "print(\"saved:\", out_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = json.loads(out_json.read_text(encoding='utf-8'))\n",
        "print(json.dumps(data, ensure_ascii=False, indent=2)[:2000])\n",
        "\n",
        "# 可选画图：延迟分布（如果你的环境有 matplotlib）\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    lat = data.get('latency_ms', {})\n",
        "    if lat:\n",
        "        xs = ['p50','p90','p99','avg']\n",
        "        ys = [lat.get('p50_ms',0), lat.get('p90_ms',0), lat.get('p99_ms',0), lat.get('avg_ms',0)]\n",
        "        plt.figure(figsize=(6,3))\n",
        "        plt.bar(xs, ys)\n",
        "        plt.title('GET /stats latency (ms)')\n",
        "        plt.ylabel('ms')\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print('matplotlib unavailable:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) （可选）“巨大计算需求”压测到 RTX demo\n",
        "\n",
        "如果你在 RTX 上启动了 `ai_demo`（端口 `18080`，路径 `/infer?work_ms=...`），就用下面脚本从本机远程压测：\n",
        "\n",
        "```bash\n",
        "python3 scripts/load_test.py --base http://10.200.98.98:18080 --path /infer --duration 30 --concurrency 400 --work-ms 800 --mode spread\n",
        "```\n",
        "\n",
        "运行后打开：\n",
        "- `http://10.200.98.98:18080/dashboard`\n",
        "\n",
        "你会在“后端 AI/GPU 指标表格 + GPU/显存/队列曲线”里看到分配与负载变化。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

