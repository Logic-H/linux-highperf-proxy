{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 远程压测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c748bbff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUT_DIR= /home/huazi/Workspace/Projects/linux大作业/notebooks/bench_out\n",
            "Python= 3.14.2\n",
            "Repo= /home/huazi/Workspace/Projects/linux大作业/notebooks\n",
            "RLIMIT_NOFILE soft=524288 hard=524288\n"
          ]
        }
      ],
      "source": [
        "import os, sys, json, time, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "import resource\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for p in [start, *start.parents]:\n",
        "        if (p / \"scripts\" / \"benchmark.py\").exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(\"找不到仓库根目录（缺少 scripts/benchmark.py）。请用仓库根目录启动 Jupyter。\")\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "\n",
        "# 目标 \n",
        "RTX_HOST = \"10.200.98.98\"\n",
        "\n",
        "PROXY_PORT = 8080\n",
        "\n",
        "AI_DEMO_PORT = 18080\n",
        "\n",
        "# 输出目录\n",
        "OUT_DIR = REPO_ROOT / \"bench_out\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"OUT_DIR=\", OUT_DIR.resolve())\n",
        "print(\"Python=\", sys.version.split()[0])\n",
        "print(\"RepoRoot=\", REPO_ROOT)\n",
        "\n",
        "nofile_soft, nofile_hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
        "print(f\"RLIMIT_NOFILE soft={nofile_soft} hard={nofile_hard}\")\n",
        "if nofile_soft < 20000:\n",
        "    print(\"WARNING: 当前进程 fd 上限偏低；10K 并发连接可能失败。建议退出后用更高 ulimit 启动 Jupyter。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8833dab",
      "metadata": {},
      "source": [
        "## 1) 快速探活：拉取 `/stats`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "306a7652",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK /stats\n",
            "uptime_sec: 2508\n",
            "active_connections: 1\n",
            "avg_qps: 0.0\n",
            "io: {'configured_model': 'epoll', 'runtime_model': 'epoll', 'supported_models': ['select', 'poll', 'epoll', 'uring']}\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "def get_json(url: str, timeout: float = 2.0):\n",
        "    with urllib.request.urlopen(url, timeout=timeout) as r:\n",
        "        data = r.read()\n",
        "    return json.loads(data.decode('utf-8'))\n",
        "\n",
        "stats_url = f\"http://{RTX_HOST}:{PROXY_PORT}/stats\"\n",
        "j = get_json(stats_url, timeout=3.0)\n",
        "print(\"OK /stats\")\n",
        "print(\"uptime_sec:\", j.get(\"uptime_sec\"))\n",
        "print(\"active_connections:\", j.get(\"active_connections\"))\n",
        "print(\"avg_qps:\", j.get(\"avg_qps\"))\n",
        "print(\"io:\", j.get(\"io\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a195abe2",
      "metadata": {},
      "source": [
        "## 2) 10K 并发\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "49bab15b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plan: total=10000 concurrency=10000 hold_s=20 waves=1 global_timeout=65s\n",
            "$ /home/huazi/Workspace/Projects/linux大作业/.venv/bin/python /home/huazi/Workspace/Projects/linux大作业/scripts/benchmark.py --host 10.200.98.98 --port 8080 --bench connect_hold --total 10000 --concurrency 10000 --hold 20 --timeout 3 --global-timeout 65 --output /home/huazi/Workspace/Projects/linux大作业/bench_out/remote_conn_hold_10k.json\n",
            "mode=epoll bench=connect_hold ok=0 failed=10000 elapsed_s=8.00 qps=0.00\n",
            "connect_hold: hold_s=20.00 concurrency=10000 total=10000\n",
            "\n",
            "saved: /home/huazi/Workspace/Projects/linux大作业/bench_out/remote_conn_hold_10k.json\n"
          ]
        }
      ],
      "source": [
        "def run_cmd(cmd: list[str], timeout_s: float | None = None) -> str:\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    p = subprocess.run(cmd, cwd=str(REPO_ROOT), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout_s)\n",
        "    print(p.stdout)\n",
        "    p.check_returncode()\n",
        "    return p.stdout\n",
        "\n",
        "# 兜底：如果你没先运行「初始化」那格，确保 REPO_ROOT/OUT_DIR 已定义。\n",
        "if \"REPO_ROOT\" not in globals():\n",
        "    from pathlib import Path\n",
        "    def _find_repo_root(start: Path) -> Path:\n",
        "        start = start.resolve()\n",
        "        for p in [start, *start.parents]:\n",
        "            if (p / \"scripts\" / \"benchmark.py\").exists():\n",
        "                return p\n",
        "        raise FileNotFoundError(\"找不到仓库根目录（缺少 scripts/benchmark.py）。请用仓库根目录启动 Jupyter。\")\n",
        "    REPO_ROOT = _find_repo_root(Path.cwd())\n",
        "    print(\"RepoRoot=\", REPO_ROOT)\n",
        "\n",
        "if \"OUT_DIR\" not in globals():\n",
        "    from pathlib import Path\n",
        "    OUT_DIR = Path(REPO_ROOT) / \"bench_out\"\n",
        "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"OUT_DIR=\", OUT_DIR.resolve())\n",
        "\n",
        "TOTAL = 10_000\n",
        "HOLD_S = 20\n",
        "CONCURRENCY = 10_000  # 真正的 10K 并发\n",
        "\n",
        "# 若本机 fd 上限不足，会直接失败；先自动降级，提示你提高 ulimit 后再跑 10K。\n",
        "if nofile_soft < CONCURRENCY + 2048:\n",
        "    print(f\"WARNING: RLIMIT_NOFILE soft={nofile_soft} 不足以支撑 concurrency={CONCURRENCY}。\")\n",
        "    print(\"建议：退出 Jupyter 后用更高的 ulimit -n 重新启动，再把 CONCURRENCY 改回 10000。\")\n",
        "    CONCURRENCY = min(2000, TOTAL)\n",
        "\n",
        "# connect_hold 会按 concurrency 分批（waves）执行；global-timeout 需要覆盖所有 waves。\n",
        "waves = (TOTAL + CONCURRENCY - 1) // CONCURRENCY\n",
        "global_timeout = int(HOLD_S * waves + 45)  # 额外留 45s 缓冲，避免 ERROR: global timeout\n",
        "subprocess_timeout = global_timeout + 30\n",
        "print(f\"plan: total={TOTAL} concurrency={CONCURRENCY} hold_s={HOLD_S} waves={waves} global_timeout={global_timeout}s\")\n",
        "\n",
        "out_json = OUT_DIR / \"remote_conn_hold_10k.json\"\n",
        "cmd = [\n",
        "    sys.executable, str(REPO_ROOT / \"scripts\" / \"benchmark.py\"),\n",
        "    \"--host\", RTX_HOST,\n",
        "    \"--port\", str(PROXY_PORT),\n",
        "    \"--bench\", \"connect_hold\",\n",
        "    \"--total\", str(TOTAL),\n",
        "    \"--concurrency\", str(CONCURRENCY),\n",
        "    \"--hold\", str(HOLD_S),\n",
        "    \"--timeout\", \"3\",\n",
        "    \"--global-timeout\", str(global_timeout),\n",
        "    \"--output\", str(out_json),\n",
        "]\n",
        "run_cmd(cmd, timeout_s=subprocess_timeout)\n",
        "print(\"saved:\", out_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf07907",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = json.loads(out_json.read_text(encoding='utf-8'))\n",
        "print(json.dumps(data, ensure_ascii=False, indent=2)[:2000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02bf2179",
      "metadata": {},
      "source": [
        "## 3) /stats 延迟与吞吐压测（http_stats）\n",
        "\n",
        "这个测试会高并发请求 RTX 的 `GET /stats`，输出 p50/p90/p99 延迟和 QPS。\n",
        "\n",
        "注意：`/stats` 不是数据面业务请求，但适合做“可观测接口在高频拉取下是否稳定”的基准。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40b0754",
      "metadata": {},
      "outputs": [],
      "source": [
        "out_json = OUT_DIR / \"remote_http_stats.json\"\n",
        "cmd = [\n",
        "    sys.executable, str(REPO_ROOT / \"scripts\" / \"benchmark.py\"),\n",
        "    \"--host\", RTX_HOST,\n",
        "    \"--port\", str(PROXY_PORT),\n",
        "    \"--bench\", \"http_stats\",\n",
        "    \"--total\", \"20000\",\n",
        "    \"--concurrency\", \"400\",\n",
        "    \"--timeout\", \"2\",\n",
        "    \"--global-timeout\", \"120\",\n",
        "    \"--output\", str(out_json),\n",
        "]\n",
        "run_cmd(cmd, timeout_s=150)\n",
        "print(\"saved:\", out_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = json.loads(out_json.read_text(encoding='utf-8'))\n",
        "print(json.dumps(data, ensure_ascii=False, indent=2)[:2000])\n",
        "\n",
        "# 可选画图：延迟分布（如果你的环境有 matplotlib）\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    lat = data.get('latency_ms', {})\n",
        "    if lat:\n",
        "        xs = ['p50','p90','p99','avg']\n",
        "        ys = [lat.get('p50_ms',0), lat.get('p90_ms',0), lat.get('p99_ms',0), lat.get('avg_ms',0)]\n",
        "        plt.figure(figsize=(6,3))\n",
        "        plt.bar(xs, ys)\n",
        "        plt.title('GET /stats latency (ms)')\n",
        "        plt.ylabel('ms')\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print('matplotlib unavailable:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) （可选）“巨大计算需求”压测到 RTX demo\n",
        "\n",
        "如果你在 RTX 上启动了 `ai_demo`（端口 `18080`，路径 `/infer?work_ms=...`），就用下面脚本从本机远程压测：\n",
        "\n",
        "```bash\n",
        "python3 scripts/load_test.py --base http://10.200.98.98:18080 --path /infer --duration 30 --concurrency 400 --work-ms 800 --mode spread\n",
        "```\n",
        "\n",
        "运行后打开：\n",
        "- `http://10.200.98.98:18080/dashboard`\n",
        "\n",
        "你会在“后端 AI/GPU 指标表格 + GPU/显存/队列曲线”里看到分配与负载变化。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
